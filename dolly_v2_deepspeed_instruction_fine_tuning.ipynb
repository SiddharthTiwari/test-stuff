{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(dolly_v2_deepspeed_instruction_finetune)=\n",
    "\n",
    "# Dolly-V2-3B Instruction Fine-Tuning wiht Ray AIR and DeepSpeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demonstration, we'll show how to use the Ray AIR for Dolly V2 3B model instruction fine-tuning using the deep-speed framework. Please uncomment the next two cells and install the following libraries dependencies.\n",
    "\n",
    "This work builds upon [existing efforts](https://github.com/ray-project/ray/blob/master/doc/source/ray-air/examples/gptj_deepspeed_fine_tuning.ipynb) by incorporating an instruction fine-tuning component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(546): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "done\n",
      "Solving environment: - \n",
      "Warning: 2 possible package resolutions (only showing differing packages):\n",
      "  - conda-forge/linux-64::mpi-1.0-openmpi, conda-forge/linux-64::openmpi-4.0.4-hdf1f1ad_0, defaults/linux-64::mpi4py-3.1.4-py310h3e5f7c9_0\n",
      "  - conda-forge/linux-64::mpi-1.0-mpich, defaults/linux-64::mpi4py-3.1.4-py310hfc96bbd_0, defaults/linux-64::mpich-3.3.2-hc856adbdone\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.7.4\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.7.4\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - mpi4py\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.7.22  |       hbcca054_0         146 KB  conda-forge\n",
      "    certifi-2023.7.22          |     pyhd8ed1ab_0         150 KB  conda-forge\n",
      "    conda-23.7.4               |  py310hff52083_0        1006 KB  conda-forge\n",
      "    libgfortran-ng-7.5.0       |      h14aa051_20          23 KB  conda-forge\n",
      "    libgfortran4-7.5.0         |      h14aa051_20         1.2 MB  conda-forge\n",
      "    mpi-1.0                    |          openmpi           4 KB  conda-forge\n",
      "    mpi4py-3.1.4               |  py310h3e5f7c9_0         576 KB\n",
      "    openmpi-4.0.4              |       hdf1f1ad_0         3.9 MB  conda-forge\n",
      "    openssl-1.1.1w             |       h7f8727e_0         3.7 MB\n",
      "    python_abi-3.10            |          2_cp310           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.5.0-h14aa051_20 \n",
      "  libgfortran4       conda-forge/linux-64::libgfortran4-7.5.0-h14aa051_20 \n",
      "  mpi                conda-forge/linux-64::mpi-1.0-openmpi \n",
      "  mpi4py             pkgs/main/linux-64::mpi4py-3.1.4-py310h3e5f7c9_0 \n",
      "  openmpi            conda-forge/linux-64::openmpi-4.0.4-hdf1f1ad_0 \n",
      "  python_abi         conda-forge/linux-64::python_abi-3.10-2_cp310 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.7.22-hbcca054_0 \n",
      "  certifi            pkgs/main/linux-64::certifi-2023.5.7-~ --> conda-forge/noarch::certifi-2023.7.22-pyhd8ed1ab_0 \n",
      "  conda              pkgs/main::conda-23.3.1-py310h06a4308~ --> conda-forge::conda-23.7.4-py310hff52083_0 \n",
      "  openssl                                 1.1.1t-h7f8727e_0 --> 1.1.1w-h7f8727e_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openmpi-4.0.4        | 3.9 MB    |                                       |   0% \n",
      "certifi-2023.7.22    | 150 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "libgfortran-ng-7.5.0 | 23 KB     |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.7.4         | 1006 KB   |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.2 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 146 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.10      | 4 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi-1.0              | 4 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 576 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.7.4         | 1006 KB   | 5                                     |   2% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libgfortran-ng-7.5.0 | 23 KB     | #########################8            |  70% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.2 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "libgfortran-ng-7.5.0 | 23 KB     | ##################################### | 100% \u001b[A\u001b[A\n",
      "certifi-2023.7.22    | 150 KB    | ###9                                  |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 146 KB    | ####                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openmpi-4.0.4        | 3.9 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "certifi-2023.7.22    | 150 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 146 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi-1.0              | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.10      | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openmpi-4.0.4        | 3.9 MB    | ###################2                  |  52% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | #################################9    |  92% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 576 KB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.7.4         | 1006 KB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.7.4         | 1006 KB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 576 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 576 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: -  \n",
      "For Linux 64, Open MPI is built with CUDA awareness but this support is disabled by default.\n",
      "To enable it, please set the environmental variable OMPI_MCA_opal_cuda_support=true before\n",
      "launching your MPI processes. Equivalently, you can set the MCA parameter in the command line:\n",
      "mpiexec --mca opal_cuda_support 1 ...\n",
      " \n",
      "\n",
      "done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - gcc\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge mpi4py -y\n",
    "!conda install gcc gxx_linux-64 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray==2.5.1\n",
      "  Downloading ray-2.5.1-cp310-cp310-manylinux2014_x86_64.whl (56.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.16.0\n",
      "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.12.0\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.26.0\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch==1.13.0\n",
      "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting deepspeed==0.9.2\n",
      "  Downloading deepspeed-0.9.2.tar.gz (779 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.3/779.3 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (4.24.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (23.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (6.0)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Downloading msgpack-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (530 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m530.8/530.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (23.1.0)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (1.4.0)\n",
      "Collecting grpcio<=1.51.3,>=1.42.0\n",
      "  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (8.1.7)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (1.24.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (2.29.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (4.17.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.5.1) (3.9.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.16.0) (5.9.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (13.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.8.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.3.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.70.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.17.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (2.1.1)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.0) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.0) (0.13.3)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0) (4.5.0)\n",
      "Collecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic<2.0.0\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (65.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.9.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.5.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.5.1) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.5.1) (3.4)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.5.1) (0.19.3)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.12.0) (1.16.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.2-py3-none-any.whl size=811217 sha256=7a6e818ec299542c1731b0814cbdccf8edb7969f159b0a39accffda435382031\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/d2/b1/b15210b5dc024bab4eccbac2148db29959fe01fe6042557d07\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, pydantic, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, msgpack, grpcio, dill, responses, ray, nvidia-cudnn-cu11, multiprocess, transformers, torch, deepspeed, datasets, accelerate\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.33.2\n",
      "    Uninstalling transformers-4.33.2:\n",
      "      Successfully uninstalled transformers-4.33.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.5\n",
      "    Uninstalling datasets-2.14.5:\n",
      "      Successfully uninstalled datasets-2.14.5\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.23.0\n",
      "    Uninstalling accelerate-0.23.0:\n",
      "      Successfully uninstalled accelerate-0.23.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "triton 2.0.0 requires cmake, which is not installed.\n",
      "triton 2.0.0 requires lit, which is not installed.\n",
      "torchdata 0.6.1 requires torch>1.13, but you have torch 1.13.0 which is incompatible.\n",
      "llama-recipes 0.0.1 requires torch>=2.0.1, but you have torch 1.13.0 which is incompatible.\n",
      "llama-recipes 0.0.1 requires transformers>=4.31.0, but you have transformers 4.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.16.0 datasets-2.12.0 deepspeed-0.9.2 dill-0.3.6 grpcio-1.51.3 hjson-3.1.0 msgpack-1.0.6 multiprocess-0.70.14 ninja-1.11.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 py-cpuinfo-9.0.0 pydantic-1.10.12 ray-2.5.1 responses-0.18.0 torch-1.13.0 transformers-4.26.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install \"ray==2.5.1\" \"accelerate==0.16.0\" \"datasets==2.12.0\" \"transformers==4.26.0\"  \"torch==1.13.0\" \"deepspeed==0.9.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.17.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.12.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (13.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import ray.data\n",
    "import ray\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from transformers.utils.logging import enable_progress_bar\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from ray.train.huggingface import TransformersTrainer\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up Ray <a name=\"setup\"></a>\n",
    "\n",
    "First, we will use 2 workers, each being assigned 1 GPU and 28 CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"databricks/dolly-v2-3b\"\n",
    "use_gpu = True\n",
    "num_workers = 1\n",
    "cpus_per_worker = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fq_ray_ip = #<replace-this-with-your-ray-server-ip-address>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_env = {\n",
    "    \"pip\": [\n",
    "        \"datasets==2.12.0\",\n",
    "        \"evaluate==0.4.0\",\n",
    "        \"accelerate==0.16.0\",  # https://github.com/OpenGVLab/InternImage/issues/111\n",
    "        \"transformers==4.26.0\",\n",
    "        \"torch==1.13.0\",\n",
    "        \"deepspeed==0.9.2\",\n",
    "        \"ipython==8.14.0\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env = {\n",
    "    \"conda\": {\n",
    "        \"dependencies\": [\"mpi4py\", \"pip\", pip_env]\n",
    "    }  # pip install mpi4py won't work, use conda install instead\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init(\n",
    "#     f\"ray://{fq_ray_ip}:10001\",  # Note: the port and ip-address depends on your ray server setup.\n",
    "#     runtime_env=conda_env,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset <a name=\"load\"></a>\n",
    "\n",
    "We will be fine-tuning the model on the [`alpaca-cleaned` dataset](https://datasets-server.huggingface.co/splits?dataset=yahma%2Falpaca-cleaned), comprised of 51,000 lines of Q&A. The aim will be to make the databricks model better at generating answer by following the instruction.\n",
    "\n",
    "We will use `generate_prompt` function to prepare our dataset for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/yahma___json/yahma--alpaca-cleaned-5d24553f76c14acc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e48b47260d148cea2425dc41f6caab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output', 'input'],\n",
       "        num_rows: 51760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "current_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # ref: https://github.com/tloen/alpaca-lora\n",
    "    if data_point[\"instruction\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = 0\n",
    "CUTOFF_LEN = 128\n",
    "\n",
    "current_dataset = current_dataset.shuffle().map(\n",
    "    lambda data_point: tokenizer(\n",
    "        generate_prompt(data_point),\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=\"max_length\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 19:37:15,152\tWARNING utils.py:593 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2023-09-21 19:37:15,154\tWARNING utils.py:605 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 15.36 to 15.\n",
      "2023-09-21 19:37:15,232\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 19:37:20,765\tWARNING dataset.py:253 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332aae10d8fe412bba120425744d294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MaterializedDataset(\n",
       "   num_blocks=1,\n",
       "   num_rows=51760,\n",
       "   schema={\n",
       "      instruction: string,\n",
       "      output: string,\n",
       "      input: string,\n",
       "      input_ids: list<item: int32>,\n",
       "      attention_mask: list<item: int8>\n",
       "   }\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_datasets = ray.data.from_huggingface(current_dataset[\"train\"])\n",
    "ray_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction fine-tuning the model with Ray AIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer_init_per_worker(train_dataset, eval_dataset=None, **config):\n",
    "    batch_size = config.get(\"batch_size\", 1)\n",
    "    epochs = config.get(\"epochs\", 1)\n",
    "    warmup_steps = config.get(\"warmup_steps\", 0)\n",
    "    learning_rate = config.get(\"learning_rate\", 0.00002)\n",
    "    weight_decay = config.get(\"weight_decay\", 0.01)\n",
    "\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 8,\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": \"auto\",\n",
    "                \"betas\": \"auto\",\n",
    "                \"eps\": \"auto\",\n",
    "            },\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 10,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False,\n",
    "    }\n",
    "\n",
    "    print(f\"batch_size: {batch_size}\")\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"deepspeed-dolly\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        logging_steps=1,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_train_epochs=epochs,\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=False,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=16,\n",
    "        deepspeed=deepspeed,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = 0\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=False\n",
    "        ),\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = TransformersTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    trainer_init_config={\n",
    "        \"batch_size\": 16,  # batch_size per device\n",
    "        \"epochs\": 1,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=use_gpu,\n",
    "        resources_per_worker={\n",
    "            \"GPU\": 1,\n",
    "            \"CPU\": cpus_per_worker,\n",
    "        },  # NOTE: huggingface transformers only support 1 GPU per worker.\n",
    "    ),\n",
    "    run_config=ray.air.RunConfig(\n",
    "        sync_config=ray.tune.syncer.SyncConfig(\n",
    "            sync_on_checkpoint=False  # Note: one can also set up a storage path to persist the model checkpoint to a cloud bucket\n",
    "        )\n",
    "    ),\n",
    "    datasets={\n",
    "        \"train\": ray_datasets,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we call the `~ray.train.huggingface.TransformersTrainer.fit` method to start training with Ray AIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/ray/train/base_trainer.py:581: UserWarning: Executing `.fit()` may leave less than 20% of CPUs in this cluster for Dataset execution, which can lead to resource contention or hangs. To avoid this, reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer scaling_config. See https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune for more info.\n",
      "  tuner = Tuner(\n",
      "2023-09-21 19:37:50,755\tINFO tensorboardx.py:178 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-09-21 19:37:50,756\tWARNING callback.py:144 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-21 19:39:47</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:56.94        </td></tr>\n",
       "<tr><td>Memory:      </td><td>20.8/503.6 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 13.0/15 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TransformersTrainer_59b78_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/TransformersTrainer_2023-09-21_19-37-50/TransformersTrainer_59b78_00000_0_2023-09-21_19-37-50/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TransformersTrainer_59b78_00000</td><td>ERROR   </td><td>172.17.0.4:29764</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TransformersTrainer pid=29764)\u001b[0m 2023-09-21 19:37:56,022\tWARNING dataset.py:253 -- \u001b[33mImportant: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=29764)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=29764)\u001b[0m Learn more here: https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode\u001b[0m\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=29764)\u001b[0m 2023-09-21 19:37:57,475\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['29899 (172.17.0.4)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m 2023-09-21 19:37:58,175\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=29899) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=29899) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m 2023-09-21 19:38:01,361\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m 2023-09-21 19:38:01,361\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m 2023-09-21 19:38:01,361\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m 2023-09-21 19:38:01,372\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-3, stopped daemon 139699959088896)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m batch_size: 16\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Preparing training arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 819/819 [00:00<00:00, 2.48MB/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 10.5M/5.68G [00:00<01:53, 50.1MB/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 21.0M/5.68G [00:00<01:18, 72.0MB/s]\n",
      "Downloading pytorch_model.bin:   1%|          | 31.5M/5.68G [00:00<01:09, 81.6MB/s]\n",
      "Downloading pytorch_model.bin:   1%|          | 41.9M/5.68G [00:00<01:06, 84.3MB/s]\n",
      "Downloading pytorch_model.bin:   1%|          | 52.4M/5.68G [00:00<01:05, 85.9MB/s]\n",
      "Downloading pytorch_model.bin:   1%|▏         | 73.4M/5.68G [00:00<00:57, 97.4MB/s]\n",
      "Downloading pytorch_model.bin:   1%|▏         | 83.9M/5.68G [00:00<00:58, 96.2MB/s]\n",
      "Downloading pytorch_model.bin:   2%|▏         | 94.4M/5.68G [00:01<00:59, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:   2%|▏         | 105M/5.68G [00:01<01:00, 91.8MB/s] \n",
      "Downloading pytorch_model.bin:   2%|▏         | 115M/5.68G [00:01<01:11, 78.3MB/s]\n",
      "Downloading pytorch_model.bin:   2%|▏         | 126M/5.68G [00:01<01:08, 81.7MB/s]\n",
      "Downloading pytorch_model.bin:   2%|▏         | 136M/5.68G [00:01<01:03, 86.9MB/s]\n",
      "Downloading pytorch_model.bin:   3%|▎         | 147M/5.68G [00:01<01:00, 91.4MB/s]\n",
      "Downloading pytorch_model.bin:   3%|▎         | 157M/5.68G [00:01<01:00, 91.6MB/s]\n",
      "Downloading pytorch_model.bin:   3%|▎         | 168M/5.68G [00:01<01:00, 91.7MB/s]\n",
      "Downloading pytorch_model.bin:   3%|▎         | 178M/5.68G [00:02<01:00, 91.5MB/s]\n",
      "Downloading pytorch_model.bin:   3%|▎         | 189M/5.68G [00:02<01:00, 91.1MB/s]\n",
      "Downloading pytorch_model.bin:   4%|▎         | 199M/5.68G [00:02<00:57, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:   4%|▎         | 210M/5.68G [00:02<00:56, 96.5MB/s]\n",
      "Downloading pytorch_model.bin:   4%|▍         | 220M/5.68G [00:02<00:57, 95.7MB/s]\n",
      "Downloading pytorch_model.bin:   4%|▍         | 231M/5.68G [00:02<01:20, 67.7MB/s]\n",
      "Downloading pytorch_model.bin:   4%|▍         | 252M/5.68G [00:02<01:06, 81.6MB/s]\n",
      "Downloading pytorch_model.bin:   5%|▍         | 273M/5.68G [00:03<01:00, 88.8MB/s]\n",
      "Downloading pytorch_model.bin:   5%|▍         | 283M/5.68G [00:03<00:58, 91.6MB/s]\n",
      "Downloading pytorch_model.bin:   5%|▌         | 294M/5.68G [00:03<00:58, 91.8MB/s]\n",
      "Downloading pytorch_model.bin:   5%|▌         | 304M/5.68G [00:03<00:58, 91.3MB/s]\n",
      "Downloading pytorch_model.bin:   6%|▌         | 315M/5.68G [00:03<00:57, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:   6%|▌         | 325M/5.68G [00:03<00:57, 93.7MB/s]\n",
      "Downloading pytorch_model.bin:   6%|▌         | 336M/5.68G [00:03<00:57, 93.3MB/s]\n",
      "Downloading pytorch_model.bin:   6%|▋         | 357M/5.68G [00:04<01:12, 73.1MB/s]\n",
      "Downloading pytorch_model.bin:   6%|▋         | 367M/5.68G [00:04<01:09, 77.0MB/s]\n",
      "Downloading pytorch_model.bin:   7%|▋         | 377M/5.68G [00:04<01:05, 80.5MB/s]\n",
      "Downloading pytorch_model.bin:   7%|▋         | 388M/5.68G [00:04<01:01, 85.7MB/s]\n",
      "Downloading pytorch_model.bin:   7%|▋         | 398M/5.68G [00:04<01:00, 87.5MB/s]\n",
      "Downloading pytorch_model.bin:   7%|▋         | 409M/5.68G [00:04<00:57, 91.4MB/s]\n",
      "Downloading pytorch_model.bin:   7%|▋         | 419M/5.68G [00:04<00:56, 92.4MB/s]\n",
      "Downloading pytorch_model.bin:   8%|▊         | 430M/5.68G [00:04<00:55, 95.4MB/s]\n",
      "Downloading pytorch_model.bin:   8%|▊         | 440M/5.68G [00:05<00:53, 98.0MB/s]\n",
      "Downloading pytorch_model.bin:   8%|▊         | 451M/5.68G [00:05<00:52, 99.8MB/s]\n",
      "Downloading pytorch_model.bin:   8%|▊         | 461M/5.68G [00:05<00:54, 96.3MB/s]\n",
      "Downloading pytorch_model.bin:   8%|▊         | 482M/5.68G [00:05<00:54, 95.6MB/s]\n",
      "Downloading pytorch_model.bin:   9%|▊         | 493M/5.68G [00:05<00:59, 87.5MB/s]\n",
      "Downloading pytorch_model.bin:   9%|▉         | 503M/5.68G [00:05<00:58, 88.8MB/s]\n",
      "Downloading pytorch_model.bin:   9%|▉         | 514M/5.68G [00:05<00:56, 92.2MB/s]\n",
      "Downloading pytorch_model.bin:   9%|▉         | 524M/5.68G [00:05<00:54, 95.0MB/s]\n",
      "Downloading pytorch_model.bin:   9%|▉         | 535M/5.68G [00:06<00:53, 96.9MB/s]\n",
      "Downloading pytorch_model.bin:  10%|▉         | 545M/5.68G [00:06<00:53, 95.6MB/s]\n",
      "Downloading pytorch_model.bin:  10%|▉         | 556M/5.68G [00:06<00:54, 94.4MB/s]\n",
      "Downloading pytorch_model.bin:  10%|█         | 577M/5.68G [00:06<00:51, 99.9MB/s]\n",
      "Downloading pytorch_model.bin:  10%|█         | 587M/5.68G [00:06<00:50, 101MB/s] \n",
      "Downloading pytorch_model.bin:  11%|█         | 598M/5.68G [00:06<00:51, 99.4MB/s]\n",
      "Downloading pytorch_model.bin:  11%|█         | 608M/5.68G [00:06<01:11, 71.4MB/s]\n",
      "Downloading pytorch_model.bin:  11%|█         | 619M/5.68G [00:07<01:08, 73.5MB/s]\n",
      "Downloading pytorch_model.bin:  11%|█         | 629M/5.68G [00:07<01:04, 78.0MB/s]\n",
      "Downloading pytorch_model.bin:  11%|█▏        | 640M/5.68G [00:07<01:00, 83.9MB/s]\n",
      "Downloading pytorch_model.bin:  12%|█▏        | 661M/5.68G [00:07<00:54, 92.7MB/s]\n",
      "Downloading pytorch_model.bin:  12%|█▏        | 671M/5.68G [00:07<00:54, 92.1MB/s]\n",
      "Downloading pytorch_model.bin:  12%|█▏        | 682M/5.68G [00:07<00:52, 95.0MB/s]\n",
      "Downloading pytorch_model.bin:  12%|█▏        | 692M/5.68G [00:07<00:51, 97.1MB/s]\n",
      "Downloading pytorch_model.bin:  12%|█▏        | 703M/5.68G [00:07<00:53, 93.6MB/s]\n",
      "Downloading pytorch_model.bin:  13%|█▎        | 713M/5.68G [00:08<00:52, 95.2MB/s]\n",
      "Downloading pytorch_model.bin:  13%|█▎        | 724M/5.68G [00:08<00:52, 93.7MB/s]\n",
      "Downloading pytorch_model.bin:  13%|█▎        | 734M/5.68G [00:08<00:55, 89.6MB/s]\n",
      "Downloading pytorch_model.bin:  13%|█▎        | 744M/5.68G [00:08<00:53, 93.0MB/s]\n",
      "Downloading pytorch_model.bin:  13%|█▎        | 755M/5.68G [00:08<00:53, 92.9MB/s]\n",
      "Downloading pytorch_model.bin:  13%|█▎        | 765M/5.68G [00:08<00:53, 92.7MB/s]\n",
      "Downloading pytorch_model.bin:  14%|█▎        | 776M/5.68G [00:08<00:52, 92.7MB/s]\n",
      "Downloading pytorch_model.bin:  14%|█▍        | 786M/5.68G [00:08<00:53, 92.0MB/s]\n",
      "Downloading pytorch_model.bin:  14%|█▍        | 807M/5.68G [00:09<00:51, 94.8MB/s]\n",
      "Downloading pytorch_model.bin:  14%|█▍        | 818M/5.68G [00:09<00:50, 96.9MB/s]\n",
      "Downloading pytorch_model.bin:  15%|█▍        | 828M/5.68G [00:09<00:50, 96.3MB/s]\n",
      "Downloading pytorch_model.bin:  15%|█▍        | 839M/5.68G [00:09<00:51, 94.9MB/s]\n",
      "Downloading pytorch_model.bin:  15%|█▍        | 849M/5.68G [00:09<00:52, 92.4MB/s]\n",
      "Downloading pytorch_model.bin:  15%|█▌        | 860M/5.68G [00:09<00:50, 95.4MB/s]\n",
      "Downloading pytorch_model.bin:  15%|█▌        | 870M/5.68G [00:09<00:50, 95.0MB/s]\n",
      "Downloading pytorch_model.bin:  16%|█▌        | 891M/5.68G [00:09<00:48, 99.2MB/s]\n",
      "Downloading pytorch_model.bin:  16%|█▌        | 902M/5.68G [00:09<00:49, 96.5MB/s]\n",
      "Downloading pytorch_model.bin:  16%|█▌        | 923M/5.68G [00:10<00:49, 96.9MB/s]\n",
      "Downloading pytorch_model.bin:  16%|█▋        | 933M/5.68G [00:10<00:48, 98.3MB/s]\n",
      "Downloading pytorch_model.bin:  17%|█▋        | 944M/5.68G [00:10<00:49, 96.3MB/s]\n",
      "Downloading pytorch_model.bin:  17%|█▋        | 954M/5.68G [00:10<00:48, 98.3MB/s]\n",
      "Downloading pytorch_model.bin:  17%|█▋        | 965M/5.68G [00:10<00:49, 96.2MB/s]\n",
      "Downloading pytorch_model.bin:  17%|█▋        | 986M/5.68G [00:11<01:08, 68.9MB/s]\n",
      "Downloading pytorch_model.bin:  18%|█▊        | 1.01G/5.68G [00:11<01:00, 77.7MB/s]\n",
      "Downloading pytorch_model.bin:  18%|█▊        | 1.03G/5.68G [00:11<00:54, 85.5MB/s]\n",
      "Downloading pytorch_model.bin:  18%|█▊        | 1.04G/5.68G [00:11<00:52, 88.2MB/s]\n",
      "Downloading pytorch_model.bin:  18%|█▊        | 1.05G/5.68G [00:11<00:51, 90.1MB/s]\n",
      "Downloading pytorch_model.bin:  19%|█▉        | 1.07G/5.68G [00:11<00:48, 95.5MB/s]\n",
      "Downloading pytorch_model.bin:  19%|█▉        | 1.08G/5.68G [00:12<01:08, 66.9MB/s]\n",
      "Downloading pytorch_model.bin:  19%|█▉        | 1.09G/5.68G [00:12<01:03, 72.8MB/s]\n",
      "Downloading pytorch_model.bin:  19%|█▉        | 1.10G/5.68G [00:12<01:07, 67.6MB/s]\n",
      "Downloading pytorch_model.bin:  20%|█▉        | 1.11G/5.68G [00:12<01:01, 74.5MB/s]\n",
      "Downloading pytorch_model.bin:  20%|█▉        | 1.13G/5.68G [00:12<00:53, 85.2MB/s]\n",
      "Downloading pytorch_model.bin:  20%|██        | 1.14G/5.68G [00:12<00:51, 88.8MB/s]\n",
      "Downloading pytorch_model.bin:  20%|██        | 1.15G/5.68G [00:13<00:50, 89.4MB/s]\n",
      "Downloading pytorch_model.bin:  20%|██        | 1.16G/5.68G [00:13<00:50, 90.1MB/s]\n",
      "Downloading pytorch_model.bin:  21%|██        | 1.17G/5.68G [00:13<00:50, 89.8MB/s]\n",
      "Downloading pytorch_model.bin:  21%|██        | 1.20G/5.68G [00:13<00:46, 96.2MB/s]\n",
      "Downloading pytorch_model.bin:  21%|██        | 1.21G/5.68G [00:13<00:46, 96.5MB/s]\n",
      "Downloading pytorch_model.bin:  21%|██▏       | 1.22G/5.68G [00:13<00:47, 95.0MB/s]\n",
      "Downloading pytorch_model.bin:  22%|██▏       | 1.23G/5.68G [00:13<00:47, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:  22%|██▏       | 1.24G/5.68G [00:13<00:47, 94.2MB/s]\n",
      "Downloading pytorch_model.bin:  22%|██▏       | 1.25G/5.68G [00:14<01:01, 72.6MB/s]\n",
      "Downloading pytorch_model.bin:  22%|██▏       | 1.27G/5.68G [00:14<00:54, 81.6MB/s]\n",
      "Downloading pytorch_model.bin:  23%|██▎       | 1.29G/5.68G [00:14<00:50, 86.8MB/s]\n",
      "Downloading pytorch_model.bin:  23%|██▎       | 1.30G/5.68G [00:14<00:49, 87.8MB/s]\n",
      "Downloading pytorch_model.bin:  23%|██▎       | 1.31G/5.68G [00:14<00:48, 91.0MB/s]\n",
      "Downloading pytorch_model.bin:  23%|██▎       | 1.32G/5.68G [00:14<00:46, 94.0MB/s]\n",
      "Downloading pytorch_model.bin:  23%|██▎       | 1.33G/5.68G [00:14<00:45, 96.5MB/s]\n",
      "Downloading pytorch_model.bin:  24%|██▎       | 1.34G/5.68G [00:15<00:45, 94.7MB/s]\n",
      "Downloading pytorch_model.bin:  24%|██▍       | 1.35G/5.68G [00:15<00:46, 93.5MB/s]\n",
      "Downloading pytorch_model.bin:  24%|██▍       | 1.37G/5.68G [00:15<01:00, 71.2MB/s]\n",
      "Downloading pytorch_model.bin:  24%|██▍       | 1.38G/5.68G [00:15<00:56, 75.7MB/s]\n",
      "Downloading pytorch_model.bin:  25%|██▍       | 1.41G/5.68G [00:15<00:52, 81.5MB/s]\n",
      "Downloading pytorch_model.bin:  25%|██▌       | 1.43G/5.68G [00:16<00:49, 86.7MB/s]\n",
      "Downloading pytorch_model.bin:  25%|██▌       | 1.45G/5.68G [00:16<00:47, 90.0MB/s]\n",
      "Downloading pytorch_model.bin:  26%|██▌       | 1.47G/5.68G [00:16<00:44, 94.5MB/s]\n",
      "Downloading pytorch_model.bin:  26%|██▌       | 1.48G/5.68G [00:16<00:44, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:  26%|██▋       | 1.50G/5.68G [00:17<00:58, 71.4MB/s]\n",
      "Downloading pytorch_model.bin:  27%|██▋       | 1.52G/5.68G [00:17<00:52, 79.6MB/s]\n",
      "Downloading pytorch_model.bin:  27%|██▋       | 1.53G/5.68G [00:17<00:49, 83.2MB/s]\n",
      "Downloading pytorch_model.bin:  27%|██▋       | 1.54G/5.68G [00:17<00:48, 84.8MB/s]\n",
      "Downloading pytorch_model.bin:  27%|██▋       | 1.55G/5.68G [00:17<00:47, 86.4MB/s]\n",
      "Downloading pytorch_model.bin:  27%|██▋       | 1.56G/5.68G [00:17<00:45, 90.3MB/s]\n",
      "Downloading pytorch_model.bin:  28%|██▊       | 1.58G/5.68G [00:17<00:42, 96.2MB/s]\n",
      "Downloading pytorch_model.bin:  28%|██▊       | 1.59G/5.68G [00:18<00:41, 97.6MB/s]\n",
      "Downloading pytorch_model.bin:  28%|██▊       | 1.60G/5.68G [00:18<00:42, 96.3MB/s]\n",
      "Downloading pytorch_model.bin:  28%|██▊       | 1.61G/5.68G [00:18<00:46, 86.8MB/s]\n",
      "Downloading pytorch_model.bin:  29%|██▊       | 1.63G/5.68G [00:18<01:11, 56.5MB/s]\n",
      "Downloading pytorch_model.bin:  29%|██▉       | 1.65G/5.68G [00:18<00:57, 70.3MB/s]\n",
      "Downloading pytorch_model.bin:  29%|██▉       | 1.66G/5.68G [00:18<00:53, 74.9MB/s]\n",
      "Downloading pytorch_model.bin:  29%|██▉       | 1.67G/5.68G [00:19<00:51, 78.1MB/s]\n",
      "Downloading pytorch_model.bin:  30%|██▉       | 1.68G/5.68G [00:19<00:48, 82.2MB/s]\n",
      "Downloading pytorch_model.bin:  30%|██▉       | 1.70G/5.68G [00:19<00:43, 90.6MB/s]\n",
      "Downloading pytorch_model.bin:  30%|███       | 1.71G/5.68G [00:19<00:42, 93.2MB/s]\n",
      "Downloading pytorch_model.bin:  30%|███       | 1.72G/5.68G [00:19<00:42, 93.6MB/s]\n",
      "Downloading pytorch_model.bin:  30%|███       | 1.73G/5.68G [00:19<00:41, 95.0MB/s]\n",
      "Downloading pytorch_model.bin:  31%|███       | 1.74G/5.68G [00:19<00:49, 79.7MB/s]\n",
      "Downloading pytorch_model.bin:  31%|███       | 1.75G/5.68G [00:20<01:03, 62.4MB/s]\n",
      "Downloading pytorch_model.bin:  31%|███       | 1.76G/5.68G [00:20<00:55, 70.5MB/s]\n",
      "Downloading pytorch_model.bin:  31%|███       | 1.77G/5.68G [00:20<00:51, 75.6MB/s]\n",
      "Downloading pytorch_model.bin:  31%|███▏      | 1.78G/5.68G [00:20<00:47, 82.1MB/s]\n",
      "Downloading pytorch_model.bin:  32%|███▏      | 1.80G/5.68G [00:20<00:43, 88.3MB/s]\n",
      "Downloading pytorch_model.bin:  32%|███▏      | 1.81G/5.68G [00:20<00:42, 91.4MB/s]\n",
      "Downloading pytorch_model.bin:  32%|███▏      | 1.82G/5.68G [00:20<00:42, 91.0MB/s]\n",
      "Downloading pytorch_model.bin:  32%|███▏      | 1.85G/5.68G [00:21<00:39, 96.2MB/s]\n",
      "Downloading pytorch_model.bin:  33%|███▎      | 1.86G/5.68G [00:21<00:39, 97.7MB/s]\n",
      "Downloading pytorch_model.bin:  33%|███▎      | 1.87G/5.68G [00:21<00:43, 87.7MB/s]\n",
      "Downloading pytorch_model.bin:  33%|███▎      | 1.88G/5.68G [00:21<00:54, 70.4MB/s]\n",
      "Downloading pytorch_model.bin:  33%|███▎      | 1.89G/5.68G [00:21<00:50, 75.0MB/s]\n",
      "Downloading pytorch_model.bin:  33%|███▎      | 1.90G/5.68G [00:21<00:48, 78.4MB/s]\n",
      "Downloading pytorch_model.bin:  34%|███▍      | 1.92G/5.68G [00:22<00:42, 88.3MB/s]\n",
      "Downloading pytorch_model.bin:  34%|███▍      | 1.93G/5.68G [00:22<00:41, 91.4MB/s]\n",
      "Downloading pytorch_model.bin:  34%|███▍      | 1.94G/5.68G [00:22<00:40, 93.0MB/s]\n",
      "Downloading pytorch_model.bin:  34%|███▍      | 1.95G/5.68G [00:22<00:41, 90.6MB/s]\n",
      "Downloading pytorch_model.bin:  35%|███▍      | 1.97G/5.68G [00:22<00:39, 94.2MB/s]\n",
      "Downloading pytorch_model.bin:  35%|███▌      | 1.99G/5.68G [00:22<00:39, 92.9MB/s]\n",
      "Downloading pytorch_model.bin:  35%|███▌      | 2.00G/5.68G [00:23<00:45, 81.0MB/s]\n",
      "Downloading pytorch_model.bin:  35%|███▌      | 2.01G/5.68G [00:23<00:43, 85.2MB/s]\n",
      "Downloading pytorch_model.bin:  36%|███▌      | 2.02G/5.68G [00:23<00:41, 89.1MB/s]\n",
      "Downloading pytorch_model.bin:  36%|███▌      | 2.03G/5.68G [00:23<00:40, 89.6MB/s]\n",
      "Downloading pytorch_model.bin:  36%|███▌      | 2.04G/5.68G [00:23<00:40, 88.8MB/s]\n",
      "Downloading pytorch_model.bin:  36%|███▌      | 2.06G/5.68G [00:23<00:39, 92.6MB/s]\n",
      "Downloading pytorch_model.bin:  36%|███▋      | 2.07G/5.68G [00:23<00:40, 89.2MB/s]\n",
      "Downloading pytorch_model.bin:  37%|███▋      | 2.09G/5.68G [00:23<00:38, 94.0MB/s]\n",
      "Downloading pytorch_model.bin:  37%|███▋      | 2.10G/5.68G [00:24<00:38, 93.5MB/s]\n",
      "Downloading pytorch_model.bin:  37%|███▋      | 2.11G/5.68G [00:24<00:38, 93.5MB/s]\n",
      "Downloading pytorch_model.bin:  37%|███▋      | 2.12G/5.68G [00:24<00:37, 96.1MB/s]\n",
      "Downloading pytorch_model.bin:  37%|███▋      | 2.13G/5.68G [00:24<00:49, 72.3MB/s]\n",
      "Downloading pytorch_model.bin:  38%|███▊      | 2.14G/5.68G [00:24<00:50, 70.4MB/s]\n",
      "Downloading pytorch_model.bin:  38%|███▊      | 2.15G/5.68G [00:24<00:45, 77.4MB/s]\n",
      "Downloading pytorch_model.bin:  38%|███▊      | 2.16G/5.68G [00:24<00:44, 79.1MB/s]\n",
      "Downloading pytorch_model.bin:  38%|███▊      | 2.17G/5.68G [00:24<00:42, 82.3MB/s]\n",
      "Downloading pytorch_model.bin:  38%|███▊      | 2.18G/5.68G [00:25<00:40, 87.3MB/s]\n",
      "Downloading pytorch_model.bin:  39%|███▊      | 2.19G/5.68G [00:25<00:38, 91.6MB/s]\n",
      "Downloading pytorch_model.bin:  39%|███▊      | 2.20G/5.68G [00:25<00:37, 92.0MB/s]\n",
      "Downloading pytorch_model.bin:  39%|███▉      | 2.21G/5.68G [00:25<00:36, 95.2MB/s]\n",
      "Downloading pytorch_model.bin:  39%|███▉      | 2.22G/5.68G [00:25<00:36, 94.2MB/s]\n",
      "Downloading pytorch_model.bin:  39%|███▉      | 2.23G/5.68G [00:25<00:36, 93.5MB/s]\n",
      "Downloading pytorch_model.bin:  40%|███▉      | 2.25G/5.68G [00:25<00:41, 82.7MB/s]\n",
      "Downloading pytorch_model.bin:  40%|███▉      | 2.26G/5.68G [00:26<00:47, 71.7MB/s]\n",
      "Downloading pytorch_model.bin:  40%|████      | 2.29G/5.68G [00:26<00:41, 82.4MB/s]\n",
      "Downloading pytorch_model.bin:  40%|████      | 2.30G/5.68G [00:26<00:40, 84.4MB/s]\n",
      "Downloading pytorch_model.bin:  41%|████      | 2.32G/5.68G [00:26<00:36, 91.6MB/s]\n",
      "Downloading pytorch_model.bin:  41%|████      | 2.33G/5.68G [00:26<00:36, 91.8MB/s]\n",
      "Downloading pytorch_model.bin:  41%|████      | 2.34G/5.68G [00:26<00:35, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:  42%|████▏     | 2.36G/5.68G [00:27<00:34, 95.8MB/s]\n",
      "Downloading pytorch_model.bin:  42%|████▏     | 2.37G/5.68G [00:27<00:34, 96.6MB/s]\n",
      "Downloading pytorch_model.bin:  42%|████▏     | 2.38G/5.68G [00:27<00:34, 96.5MB/s]\n",
      "Downloading pytorch_model.bin:  42%|████▏     | 2.39G/5.68G [00:27<00:34, 95.4MB/s]\n",
      "Downloading pytorch_model.bin:  42%|████▏     | 2.40G/5.68G [00:27<00:35, 91.8MB/s]\n",
      "Downloading pytorch_model.bin:  42%|████▏     | 2.41G/5.68G [00:27<00:46, 70.4MB/s]\n",
      "Downloading pytorch_model.bin:  43%|████▎     | 2.42G/5.68G [00:27<00:42, 77.4MB/s]\n",
      "Downloading pytorch_model.bin:  43%|████▎     | 2.43G/5.68G [00:28<00:47, 68.4MB/s]\n",
      "Downloading pytorch_model.bin:  43%|████▎     | 2.44G/5.68G [00:28<00:56, 57.2MB/s]\n",
      "Downloading pytorch_model.bin:  43%|████▎     | 2.45G/5.68G [00:28<00:50, 64.2MB/s]\n",
      "Downloading pytorch_model.bin:  44%|████▎     | 2.47G/5.68G [00:28<00:40, 78.3MB/s]\n",
      "Downloading pytorch_model.bin:  44%|████▎     | 2.49G/5.68G [00:28<00:39, 80.8MB/s]\n",
      "Downloading pytorch_model.bin:  44%|████▍     | 2.50G/5.68G [00:28<00:37, 85.6MB/s]\n",
      "Downloading pytorch_model.bin:  44%|████▍     | 2.51G/5.68G [00:28<00:35, 89.9MB/s]\n",
      "Downloading pytorch_model.bin:  44%|████▍     | 2.52G/5.68G [00:29<00:33, 93.2MB/s]\n",
      "Downloading pytorch_model.bin:  44%|████▍     | 2.53G/5.68G [00:29<00:34, 92.6MB/s]\n",
      "Downloading pytorch_model.bin:  45%|████▍     | 2.54G/5.68G [00:29<00:32, 95.8MB/s]\n",
      "Downloading pytorch_model.bin:  45%|████▍     | 2.55G/5.68G [00:29<00:32, 97.7MB/s]\n",
      "Downloading pytorch_model.bin:  45%|████▌     | 2.56G/5.68G [00:29<00:32, 96.8MB/s]\n",
      "Downloading pytorch_model.bin:  45%|████▌     | 2.57G/5.68G [00:29<00:31, 97.6MB/s]\n",
      "Downloading pytorch_model.bin:  45%|████▌     | 2.58G/5.68G [00:29<00:32, 96.0MB/s]\n",
      "Downloading pytorch_model.bin:  46%|████▌     | 2.59G/5.68G [00:29<00:31, 98.2MB/s]\n",
      "Downloading pytorch_model.bin:  46%|████▌     | 2.60G/5.68G [00:29<00:30, 99.6MB/s]\n",
      "Downloading pytorch_model.bin:  46%|████▌     | 2.61G/5.68G [00:29<00:31, 97.4MB/s]\n",
      "Downloading pytorch_model.bin:  46%|████▌     | 2.62G/5.68G [00:30<00:42, 72.8MB/s]\n",
      "Downloading pytorch_model.bin:  46%|████▋     | 2.63G/5.68G [00:30<00:38, 79.7MB/s]\n",
      "Downloading pytorch_model.bin:  46%|████▋     | 2.64G/5.68G [00:30<00:36, 83.1MB/s]\n",
      "Downloading pytorch_model.bin:  47%|████▋     | 2.65G/5.68G [00:30<00:34, 88.3MB/s]\n",
      "Downloading pytorch_model.bin:  47%|████▋     | 2.66G/5.68G [00:30<00:33, 89.5MB/s]\n",
      "Downloading pytorch_model.bin:  47%|████▋     | 2.68G/5.68G [00:30<00:31, 95.7MB/s]\n",
      "Downloading pytorch_model.bin:  47%|████▋     | 2.69G/5.68G [00:30<00:31, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:  48%|████▊     | 2.71G/5.68G [00:31<00:31, 93.3MB/s]\n",
      "Downloading pytorch_model.bin:  48%|████▊     | 2.72G/5.68G [00:31<00:31, 95.5MB/s]\n",
      "Downloading pytorch_model.bin:  48%|████▊     | 2.73G/5.68G [00:31<00:31, 94.3MB/s]\n",
      "Downloading pytorch_model.bin:  48%|████▊     | 2.74G/5.68G [00:31<00:34, 85.5MB/s]\n",
      "Downloading pytorch_model.bin:  48%|████▊     | 2.75G/5.68G [00:31<00:33, 87.4MB/s]\n",
      "Downloading pytorch_model.bin:  49%|████▊     | 2.76G/5.68G [00:31<00:32, 88.9MB/s]\n",
      "Downloading pytorch_model.bin:  49%|████▊     | 2.77G/5.68G [00:31<00:32, 90.7MB/s]\n",
      "Downloading pytorch_model.bin:  49%|████▉     | 2.78G/5.68G [00:31<00:31, 92.9MB/s]\n",
      "Downloading pytorch_model.bin:  49%|████▉     | 2.79G/5.68G [00:31<00:30, 95.7MB/s]\n",
      "Downloading pytorch_model.bin:  49%|████▉     | 2.80G/5.68G [00:32<00:30, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:  49%|████▉     | 2.81G/5.68G [00:32<00:29, 96.9MB/s]\n",
      "Downloading pytorch_model.bin:  50%|████▉     | 2.82G/5.68G [00:32<00:29, 95.5MB/s]\n",
      "Downloading pytorch_model.bin:  50%|████▉     | 2.83G/5.68G [00:32<00:30, 94.0MB/s]\n",
      "Downloading pytorch_model.bin:  50%|████▉     | 2.84G/5.68G [00:32<00:30, 93.1MB/s]\n",
      "Downloading pytorch_model.bin:  50%|█████     | 2.85G/5.68G [00:32<00:44, 63.9MB/s]\n",
      "Downloading pytorch_model.bin:  50%|█████     | 2.86G/5.68G [00:32<00:41, 68.8MB/s]\n",
      "Downloading pytorch_model.bin:  51%|█████     | 2.87G/5.68G [00:33<00:36, 76.2MB/s]\n",
      "Downloading pytorch_model.bin:  51%|█████     | 2.88G/5.68G [00:33<00:34, 80.3MB/s]\n",
      "Downloading pytorch_model.bin:  51%|█████     | 2.89G/5.68G [00:33<00:32, 86.1MB/s]\n",
      "Downloading pytorch_model.bin:  51%|█████     | 2.90G/5.68G [00:33<00:31, 87.6MB/s]\n",
      "Downloading pytorch_model.bin:  51%|█████▏    | 2.92G/5.68G [00:33<00:30, 90.4MB/s]\n",
      "Downloading pytorch_model.bin:  51%|█████▏    | 2.93G/5.68G [00:33<00:30, 90.2MB/s]\n",
      "Downloading pytorch_model.bin:  52%|█████▏    | 2.94G/5.68G [00:33<00:29, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:  52%|█████▏    | 2.95G/5.68G [00:33<00:28, 96.6MB/s]\n",
      "Downloading pytorch_model.bin:  52%|█████▏    | 2.97G/5.68G [00:34<00:34, 78.4MB/s]\n",
      "Downloading pytorch_model.bin:  52%|█████▏    | 2.98G/5.68G [00:34<00:33, 81.3MB/s]\n",
      "Downloading pytorch_model.bin:  53%|█████▎    | 2.99G/5.68G [00:34<00:31, 86.3MB/s]\n",
      "Downloading pytorch_model.bin:  53%|█████▎    | 3.00G/5.68G [00:34<00:30, 87.6MB/s]\n",
      "Downloading pytorch_model.bin:  53%|█████▎    | 3.01G/5.68G [00:34<00:29, 91.2MB/s]\n",
      "Downloading pytorch_model.bin:  53%|█████▎    | 3.02G/5.68G [00:34<00:29, 91.2MB/s]\n",
      "Downloading pytorch_model.bin:  53%|█████▎    | 3.03G/5.68G [00:34<00:28, 94.0MB/s]\n",
      "Downloading pytorch_model.bin:  54%|█████▎    | 3.05G/5.68G [00:35<00:26, 98.2MB/s]\n",
      "Downloading pytorch_model.bin:  54%|█████▍    | 3.06G/5.68G [00:35<00:27, 96.6MB/s]\n",
      "Downloading pytorch_model.bin:  54%|█████▍    | 3.07G/5.68G [00:35<00:26, 98.6MB/s]\n",
      "Downloading pytorch_model.bin:  54%|█████▍    | 3.09G/5.68G [00:35<00:26, 98.3MB/s]\n",
      "Downloading pytorch_model.bin:  55%|█████▍    | 3.10G/5.68G [00:35<00:29, 86.8MB/s]\n",
      "Downloading pytorch_model.bin:  55%|█████▍    | 3.11G/5.68G [00:35<00:36, 71.1MB/s]\n",
      "Downloading pytorch_model.bin:  55%|█████▌    | 3.14G/5.68G [00:36<00:31, 82.0MB/s]\n",
      "Downloading pytorch_model.bin:  55%|█████▌    | 3.15G/5.68G [00:36<00:29, 84.9MB/s]\n",
      "Downloading pytorch_model.bin:  56%|█████▌    | 3.16G/5.68G [00:36<00:28, 88.0MB/s]\n",
      "Downloading pytorch_model.bin:  56%|█████▌    | 3.17G/5.68G [00:36<00:27, 91.7MB/s]\n",
      "Downloading pytorch_model.bin:  56%|█████▌    | 3.18G/5.68G [00:36<00:27, 92.0MB/s]\n",
      "Downloading pytorch_model.bin:  56%|█████▌    | 3.19G/5.68G [00:36<00:26, 95.2MB/s]\n",
      "Downloading pytorch_model.bin:  56%|█████▋    | 3.20G/5.68G [00:36<00:25, 97.7MB/s]\n",
      "Downloading pytorch_model.bin:  56%|█████▋    | 3.21G/5.68G [00:36<00:25, 96.2MB/s]\n",
      "Downloading pytorch_model.bin:  57%|█████▋    | 3.23G/5.68G [00:37<00:33, 73.7MB/s]\n",
      "Downloading pytorch_model.bin:  57%|█████▋    | 3.24G/5.68G [00:37<00:34, 69.9MB/s]\n",
      "Downloading pytorch_model.bin:  57%|█████▋    | 3.26G/5.68G [00:37<00:30, 79.3MB/s]\n",
      "Downloading pytorch_model.bin:  58%|█████▊    | 3.27G/5.68G [00:37<00:29, 81.9MB/s]\n",
      "Downloading pytorch_model.bin:  58%|█████▊    | 3.28G/5.68G [00:37<00:28, 84.9MB/s]\n",
      "Downloading pytorch_model.bin:  58%|█████▊    | 3.29G/5.68G [00:37<00:27, 86.5MB/s]\n",
      "Downloading pytorch_model.bin:  58%|█████▊    | 3.30G/5.68G [00:37<00:26, 88.3MB/s]\n",
      "Downloading pytorch_model.bin:  58%|█████▊    | 3.32G/5.68G [00:38<00:24, 95.3MB/s]\n",
      "Downloading pytorch_model.bin:  59%|█████▊    | 3.33G/5.68G [00:38<00:24, 95.7MB/s]\n",
      "Downloading pytorch_model.bin:  59%|█████▉    | 3.34G/5.68G [00:38<00:24, 94.7MB/s]\n",
      "Downloading pytorch_model.bin:  59%|█████▉    | 3.36G/5.68G [00:38<00:24, 93.8MB/s]\n",
      "Downloading pytorch_model.bin:  59%|█████▉    | 3.38G/5.68G [00:38<00:23, 98.7MB/s]\n",
      "Downloading pytorch_model.bin:  60%|█████▉    | 3.40G/5.68G [00:38<00:23, 99.3MB/s]\n",
      "Downloading pytorch_model.bin:  60%|█████▉    | 3.41G/5.68G [00:39<00:23, 96.4MB/s]\n",
      "Downloading pytorch_model.bin:  60%|██████    | 3.43G/5.68G [00:39<00:23, 97.2MB/s]\n",
      "Downloading pytorch_model.bin:  61%|██████    | 3.44G/5.68G [00:39<00:23, 96.1MB/s]\n",
      "Downloading pytorch_model.bin:  61%|██████    | 3.45G/5.68G [00:39<00:23, 95.3MB/s]\n",
      "Downloading pytorch_model.bin:  61%|██████    | 3.46G/5.68G [00:39<00:23, 93.7MB/s]\n",
      "Downloading pytorch_model.bin:  61%|██████    | 3.48G/5.68G [00:39<00:22, 98.4MB/s]\n",
      "Downloading pytorch_model.bin:  62%|██████▏   | 3.50G/5.68G [00:40<00:27, 79.6MB/s]\n",
      "Downloading pytorch_model.bin:  62%|██████▏   | 3.51G/5.68G [00:40<00:29, 74.3MB/s]\n",
      "Downloading pytorch_model.bin:  62%|██████▏   | 3.53G/5.68G [00:40<00:26, 81.7MB/s]\n",
      "Downloading pytorch_model.bin:  63%|██████▎   | 3.55G/5.68G [00:40<00:24, 85.6MB/s]\n",
      "Downloading pytorch_model.bin:  63%|██████▎   | 3.58G/5.68G [00:40<00:23, 91.4MB/s]\n",
      "Downloading pytorch_model.bin:  63%|██████▎   | 3.59G/5.68G [00:41<00:22, 91.5MB/s]\n",
      "Downloading pytorch_model.bin:  63%|██████▎   | 3.60G/5.68G [00:41<00:22, 93.7MB/s]\n",
      "Downloading pytorch_model.bin:  63%|██████▎   | 3.61G/5.68G [00:41<00:22, 93.4MB/s]\n",
      "Downloading pytorch_model.bin:  64%|██████▍   | 3.63G/5.68G [00:41<00:29, 69.0MB/s]\n",
      "Downloading pytorch_model.bin:  64%|██████▍   | 3.64G/5.68G [00:41<00:27, 73.4MB/s]\n",
      "Downloading pytorch_model.bin:  64%|██████▍   | 3.65G/5.68G [00:41<00:25, 78.8MB/s]\n",
      "Downloading pytorch_model.bin:  64%|██████▍   | 3.66G/5.68G [00:42<00:24, 82.7MB/s]\n",
      "Downloading pytorch_model.bin:  65%|██████▍   | 3.67G/5.68G [00:42<00:23, 86.6MB/s]\n",
      "Downloading pytorch_model.bin:  65%|██████▍   | 3.68G/5.68G [00:42<00:22, 88.1MB/s]\n",
      "Downloading pytorch_model.bin:  65%|██████▍   | 3.69G/5.68G [00:42<00:21, 91.8MB/s]\n",
      "Downloading pytorch_model.bin:  65%|██████▌   | 3.70G/5.68G [00:42<00:21, 90.9MB/s]\n",
      "Downloading pytorch_model.bin:  65%|██████▌   | 3.71G/5.68G [00:42<00:20, 94.3MB/s]\n",
      "Downloading pytorch_model.bin:  65%|██████▌   | 3.72G/5.68G [00:42<00:22, 88.4MB/s]\n",
      "Downloading pytorch_model.bin:  66%|██████▌   | 3.74G/5.68G [00:42<00:20, 93.2MB/s]\n",
      "Downloading pytorch_model.bin:  66%|██████▌   | 3.75G/5.68G [00:43<00:24, 80.1MB/s]\n",
      "Downloading pytorch_model.bin:  66%|██████▌   | 3.76G/5.68G [00:43<00:23, 83.0MB/s]\n",
      "Downloading pytorch_model.bin:  66%|██████▋   | 3.77G/5.68G [00:43<00:22, 84.9MB/s]\n",
      "Downloading pytorch_model.bin:  67%|██████▋   | 3.79G/5.68G [00:43<00:21, 86.5MB/s]\n",
      "Downloading pytorch_model.bin:  67%|██████▋   | 3.81G/5.68G [00:43<00:20, 93.9MB/s]\n",
      "Downloading pytorch_model.bin:  67%|██████▋   | 3.82G/5.68G [00:43<00:19, 95.9MB/s]\n",
      "Downloading pytorch_model.bin:  67%|██████▋   | 3.83G/5.68G [00:43<00:19, 93.8MB/s]\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 3.84G/5.68G [00:44<00:21, 85.9MB/s]\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 3.85G/5.68G [00:44<00:22, 83.2MB/s]\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 3.86G/5.68G [00:44<00:32, 56.9MB/s]\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 3.87G/5.68G [00:44<00:27, 65.4MB/s]\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 3.88G/5.68G [00:44<00:24, 73.3MB/s]\n",
      "Downloading pytorch_model.bin:  68%|██████▊   | 3.89G/5.68G [00:44<00:23, 77.3MB/s]\n",
      "Downloading pytorch_model.bin:  69%|██████▊   | 3.90G/5.68G [00:44<00:21, 83.2MB/s]\n",
      "Downloading pytorch_model.bin:  69%|██████▉   | 3.92G/5.68G [00:45<00:19, 90.5MB/s]\n",
      "Downloading pytorch_model.bin:  69%|██████▉   | 3.93G/5.68G [00:45<00:18, 92.3MB/s]\n",
      "Downloading pytorch_model.bin:  69%|██████▉   | 3.94G/5.68G [00:45<00:18, 94.7MB/s]\n",
      "Downloading pytorch_model.bin:  70%|██████▉   | 3.95G/5.68G [00:45<00:18, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:  70%|██████▉   | 3.96G/5.68G [00:45<00:18, 93.1MB/s]\n",
      "Downloading pytorch_model.bin:  70%|███████   | 3.98G/5.68G [00:45<00:22, 74.0MB/s]\n",
      "Downloading pytorch_model.bin:  70%|███████   | 4.00G/5.68G [00:46<00:21, 78.1MB/s]\n",
      "Downloading pytorch_model.bin:  70%|███████   | 4.01G/5.68G [00:46<00:20, 81.8MB/s]\n",
      "Downloading pytorch_model.bin:  71%|███████   | 4.02G/5.68G [00:46<00:19, 86.2MB/s]\n",
      "Downloading pytorch_model.bin:  71%|███████   | 4.03G/5.68G [00:46<00:18, 87.9MB/s]\n",
      "Downloading pytorch_model.bin:  71%|███████   | 4.04G/5.68G [00:46<00:28, 57.7MB/s]\n",
      "Downloading pytorch_model.bin:  71%|███████   | 4.05G/5.68G [00:46<00:24, 65.9MB/s]\n",
      "Downloading pytorch_model.bin:  72%|███████▏  | 4.07G/5.68G [00:46<00:20, 79.5MB/s]\n",
      "Downloading pytorch_model.bin:  72%|███████▏  | 4.09G/5.68G [00:47<00:20, 77.0MB/s]\n",
      "Downloading pytorch_model.bin:  72%|███████▏  | 4.11G/5.68G [00:47<00:18, 83.5MB/s]\n",
      "Downloading pytorch_model.bin:  72%|███████▏  | 4.12G/5.68G [00:47<00:18, 84.8MB/s]\n",
      "Downloading pytorch_model.bin:  73%|███████▎  | 4.13G/5.68G [00:47<00:17, 88.5MB/s]\n",
      "Downloading pytorch_model.bin:  73%|███████▎  | 4.14G/5.68G [00:47<00:17, 90.3MB/s]\n",
      "Downloading pytorch_model.bin:  73%|███████▎  | 4.15G/5.68G [00:47<00:16, 92.5MB/s]\n",
      "Downloading pytorch_model.bin:  73%|███████▎  | 4.16G/5.68G [00:48<00:16, 93.8MB/s]\n",
      "Downloading pytorch_model.bin:  73%|███████▎  | 4.17G/5.68G [00:48<00:15, 94.8MB/s]\n",
      "Downloading pytorch_model.bin:  74%|███████▎  | 4.18G/5.68G [00:48<00:15, 94.2MB/s]\n",
      "Downloading pytorch_model.bin:  74%|███████▍  | 4.19G/5.68G [00:48<00:16, 93.0MB/s]\n",
      "Downloading pytorch_model.bin:  74%|███████▍  | 4.22G/5.68G [00:48<00:15, 95.6MB/s]\n",
      "Downloading pytorch_model.bin:  74%|███████▍  | 4.23G/5.68G [00:48<00:18, 77.0MB/s]\n",
      "Downloading pytorch_model.bin:  75%|███████▍  | 4.24G/5.68G [00:48<00:18, 80.3MB/s]\n",
      "Downloading pytorch_model.bin:  75%|███████▍  | 4.25G/5.68G [00:49<00:17, 82.9MB/s]\n",
      "Downloading pytorch_model.bin:  75%|███████▍  | 4.26G/5.68G [00:49<00:16, 87.7MB/s]\n",
      "Downloading pytorch_model.bin:  75%|███████▌  | 4.27G/5.68G [00:49<00:15, 91.3MB/s]\n",
      "Downloading pytorch_model.bin:  75%|███████▌  | 4.29G/5.68G [00:49<00:14, 96.7MB/s]\n",
      "Downloading pytorch_model.bin:  76%|███████▌  | 4.30G/5.68G [00:49<00:14, 98.3MB/s]\n",
      "Downloading pytorch_model.bin:  76%|███████▌  | 4.32G/5.68G [00:49<00:13, 101MB/s] \n",
      "Downloading pytorch_model.bin:  76%|███████▌  | 4.33G/5.68G [00:49<00:13, 101MB/s]\n",
      "Downloading pytorch_model.bin:  76%|███████▋  | 4.34G/5.68G [00:49<00:13, 99.1MB/s]\n",
      "Downloading pytorch_model.bin:  77%|███████▋  | 4.35G/5.68G [00:50<00:15, 87.4MB/s]\n",
      "Downloading pytorch_model.bin:  77%|███████▋  | 4.36G/5.68G [00:50<00:14, 88.4MB/s]\n",
      "Downloading pytorch_model.bin:  77%|███████▋  | 4.38G/5.68G [00:50<00:14, 92.2MB/s]\n",
      "Downloading pytorch_model.bin:  77%|███████▋  | 4.39G/5.68G [00:50<00:13, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:  77%|███████▋  | 4.40G/5.68G [00:50<00:13, 93.1MB/s]\n",
      "Downloading pytorch_model.bin:  78%|███████▊  | 4.42G/5.68G [00:50<00:13, 94.9MB/s]\n",
      "Downloading pytorch_model.bin:  78%|███████▊  | 4.45G/5.68G [00:51<00:12, 96.4MB/s]\n",
      "Downloading pytorch_model.bin:  78%|███████▊  | 4.46G/5.68G [00:51<00:12, 97.7MB/s]\n",
      "Downloading pytorch_model.bin:  79%|███████▊  | 4.47G/5.68G [00:51<00:12, 96.5MB/s]\n",
      "Downloading pytorch_model.bin:  79%|███████▉  | 4.48G/5.68G [00:51<00:14, 85.3MB/s]\n",
      "Downloading pytorch_model.bin:  79%|███████▉  | 4.49G/5.68G [00:51<00:13, 86.3MB/s]\n",
      "Downloading pytorch_model.bin:  79%|███████▉  | 4.51G/5.68G [00:51<00:12, 91.3MB/s]\n",
      "Downloading pytorch_model.bin:  80%|███████▉  | 4.52G/5.68G [00:51<00:12, 91.5MB/s]\n",
      "Downloading pytorch_model.bin:  80%|███████▉  | 4.54G/5.68G [00:52<00:12, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:  80%|████████  | 4.55G/5.68G [00:52<00:11, 96.3MB/s]\n",
      "Downloading pytorch_model.bin:  80%|████████  | 4.56G/5.68G [00:52<00:11, 94.4MB/s]\n",
      "Downloading pytorch_model.bin:  80%|████████  | 4.57G/5.68G [00:52<00:11, 95.9MB/s]\n",
      "Downloading pytorch_model.bin:  81%|████████  | 4.59G/5.68G [00:52<00:11, 99.0MB/s]\n",
      "Downloading pytorch_model.bin:  81%|████████  | 4.61G/5.68G [00:52<00:10, 100MB/s] \n",
      "Downloading pytorch_model.bin:  81%|████████▏ | 4.62G/5.68G [00:53<00:13, 76.2MB/s]\n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 4.63G/5.68G [00:53<00:13, 80.5MB/s]\n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 4.65G/5.68G [00:53<00:12, 82.8MB/s]\n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 4.66G/5.68G [00:53<00:12, 84.8MB/s]\n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 4.67G/5.68G [00:53<00:11, 89.3MB/s]\n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 4.68G/5.68G [00:53<00:10, 93.0MB/s]\n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 4.69G/5.68G [00:53<00:10, 92.7MB/s]\n",
      "Downloading pytorch_model.bin:  83%|████████▎ | 4.70G/5.68G [00:53<00:10, 91.8MB/s]\n",
      "Downloading pytorch_model.bin:  83%|████████▎ | 4.71G/5.68G [00:53<00:10, 90.1MB/s]\n",
      "Downloading pytorch_model.bin:  83%|████████▎ | 4.73G/5.68G [00:54<00:10, 92.2MB/s]\n",
      "Downloading pytorch_model.bin:  83%|████████▎ | 4.74G/5.68G [00:54<00:10, 92.0MB/s]\n",
      "Downloading pytorch_model.bin:  84%|████████▎ | 4.75G/5.68G [00:54<00:14, 66.7MB/s]\n",
      "Downloading pytorch_model.bin:  84%|████████▎ | 4.76G/5.68G [00:54<00:12, 71.8MB/s]\n",
      "Downloading pytorch_model.bin:  84%|████████▍ | 4.78G/5.68G [00:54<00:10, 83.6MB/s]\n",
      "Downloading pytorch_model.bin:  84%|████████▍ | 4.80G/5.68G [00:55<00:09, 88.4MB/s]\n",
      "Downloading pytorch_model.bin:  85%|████████▍ | 4.82G/5.68G [00:55<00:09, 93.7MB/s]\n",
      "Downloading pytorch_model.bin:  85%|████████▌ | 4.84G/5.68G [00:55<00:08, 95.4MB/s]\n",
      "Downloading pytorch_model.bin:  85%|████████▌ | 4.85G/5.68G [00:55<00:08, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:  86%|████████▌ | 4.87G/5.68G [00:55<00:08, 93.4MB/s]\n",
      "Downloading pytorch_model.bin:  86%|████████▌ | 4.88G/5.68G [00:56<00:12, 64.2MB/s]\n",
      "Downloading pytorch_model.bin:  86%|████████▌ | 4.90G/5.68G [00:56<00:10, 76.0MB/s]\n",
      "Downloading pytorch_model.bin:  86%|████████▋ | 4.91G/5.68G [00:56<00:09, 79.1MB/s]\n",
      "Downloading pytorch_model.bin:  87%|████████▋ | 4.92G/5.68G [00:56<00:09, 83.8MB/s]\n",
      "Downloading pytorch_model.bin:  87%|████████▋ | 4.93G/5.68G [00:56<00:08, 85.8MB/s]\n",
      "Downloading pytorch_model.bin:  87%|████████▋ | 4.94G/5.68G [00:56<00:08, 86.5MB/s]\n",
      "Downloading pytorch_model.bin:  87%|████████▋ | 4.95G/5.68G [00:56<00:08, 90.9MB/s]\n",
      "Downloading pytorch_model.bin:  87%|████████▋ | 4.96G/5.68G [00:56<00:07, 93.9MB/s]\n",
      "Downloading pytorch_model.bin:  87%|████████▋ | 4.97G/5.68G [00:57<00:07, 96.1MB/s]\n",
      "Downloading pytorch_model.bin:  88%|████████▊ | 4.98G/5.68G [00:57<00:07, 94.9MB/s]\n",
      "Downloading pytorch_model.bin:  88%|████████▊ | 5.00G/5.68G [00:57<00:06, 98.3MB/s]\n",
      "Downloading pytorch_model.bin:  88%|████████▊ | 5.01G/5.68G [00:57<00:07, 89.7MB/s]\n",
      "Downloading pytorch_model.bin:  88%|████████▊ | 5.02G/5.68G [00:57<00:08, 80.3MB/s]\n",
      "Downloading pytorch_model.bin:  89%|████████▊ | 5.03G/5.68G [00:57<00:07, 83.1MB/s]\n",
      "Downloading pytorch_model.bin:  89%|████████▊ | 5.04G/5.68G [00:57<00:07, 85.3MB/s]\n",
      "Downloading pytorch_model.bin:  89%|████████▉ | 5.06G/5.68G [00:58<00:06, 91.3MB/s]\n",
      "Downloading pytorch_model.bin:  89%|████████▉ | 5.08G/5.68G [00:58<00:06, 92.9MB/s]\n",
      "Downloading pytorch_model.bin:  89%|████████▉ | 5.09G/5.68G [00:58<00:06, 92.9MB/s]\n",
      "Downloading pytorch_model.bin:  90%|████████▉ | 5.11G/5.68G [00:58<00:05, 97.6MB/s]\n",
      "Downloading pytorch_model.bin:  90%|█████████ | 5.12G/5.68G [00:58<00:05, 96.3MB/s]\n",
      "Downloading pytorch_model.bin:  90%|█████████ | 5.13G/5.68G [00:58<00:05, 98.0MB/s]\n",
      "Downloading pytorch_model.bin:  90%|█████████ | 5.14G/5.68G [00:58<00:05, 99.3MB/s]\n",
      "Downloading pytorch_model.bin:  91%|█████████ | 5.15G/5.68G [00:59<00:06, 78.0MB/s]\n",
      "Downloading pytorch_model.bin:  91%|█████████ | 5.16G/5.68G [00:59<00:06, 81.5MB/s]\n",
      "Downloading pytorch_model.bin:  91%|█████████ | 5.17G/5.68G [00:59<00:06, 84.0MB/s]\n",
      "Downloading pytorch_model.bin:  91%|█████████▏| 5.19G/5.68G [00:59<00:05, 89.5MB/s]\n",
      "Downloading pytorch_model.bin:  91%|█████████▏| 5.20G/5.68G [00:59<00:05, 92.7MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 5.21G/5.68G [00:59<00:04, 95.2MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 5.22G/5.68G [00:59<00:04, 95.5MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 5.23G/5.68G [00:59<00:04, 96.1MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 5.24G/5.68G [01:00<00:04, 95.1MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 5.26G/5.68G [01:00<00:04, 95.3MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 5.27G/5.68G [01:00<00:04, 86.7MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 5.28G/5.68G [01:00<00:04, 87.5MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 5.31G/5.68G [01:00<00:04, 94.5MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▎| 5.32G/5.68G [01:00<00:03, 93.5MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 5.34G/5.68G [01:01<00:03, 94.9MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 5.35G/5.68G [01:01<00:03, 93.9MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 5.37G/5.68G [01:01<00:03, 95.5MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▍| 5.39G/5.68G [01:01<00:03, 79.5MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▍| 5.40G/5.68G [01:01<00:03, 76.1MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▌| 5.41G/5.68G [01:02<00:03, 80.8MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▌| 5.42G/5.68G [01:02<00:03, 83.5MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 5.43G/5.68G [01:02<00:02, 87.8MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 5.44G/5.68G [01:02<00:02, 91.8MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 5.45G/5.68G [01:02<00:02, 95.1MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 5.46G/5.68G [01:02<00:02, 97.6MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▋| 5.48G/5.68G [01:02<00:01, 101MB/s] \n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 5.49G/5.68G [01:02<00:01, 98.8MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 5.52G/5.68G [01:03<00:01, 98.2MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 5.53G/5.68G [01:03<00:01, 98.2MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 5.55G/5.68G [01:03<00:01, 100MB/s] \n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 5.56G/5.68G [01:03<00:01, 94.9MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 5.58G/5.68G [01:03<00:01, 96.2MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 5.59G/5.68G [01:03<00:01, 95.3MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▊| 5.60G/5.68G [01:03<00:00, 96.1MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▊| 5.61G/5.68G [01:04<00:00, 94.7MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 5.62G/5.68G [01:04<00:00, 95.9MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 5.63G/5.68G [01:04<00:00, 61.6MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 5.64G/5.68G [01:04<00:00, 69.2MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 5.65G/5.68G [01:04<00:00, 63.9MB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████▉| 5.66G/5.68G [01:04<00:00, 69.9MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 5.68G/5.68G [01:05<00:00, 87.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m [2023-09-21 19:39:16,162] [INFO] [partition_parameters.py:454:__exit__] finished initializing model with 2.78B parameters\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m [2023-09-21 19:39:20,301] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.2, git-hash=unknown, git-branch=unknown\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m [2023-09-21 19:39:20,314] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m [2023-09-21 19:39:21,460] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m [1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m \u001b[31mFAILED: \u001b[0mcustom_cuda_kernel.cuda.o \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m nvcc fatal   : Unsupported gpu architecture 'compute_89'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Creating extension directory /root/.cache/torch_extensions/py310_cu117/cpu_adam...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Detected CUDA files, patching ldflags\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Emitting ninja build file /root/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Building extension module cpu_adam...\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "2023-09-21 19:39:47,693\tERROR tune_controller.py:873 -- Trial task failed for trial TransformersTrainer_59b78_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=29764, ip=172.17.0.4, actor_id=998d5bc38c16d678c5d43ca701000000, repr=TransformersTrainer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=29899, ip=172.17.0.4, actor_id=048306667ae97d4c85a482aa01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f0f70152560>)\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 526, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=29899, ip=172.17.0.4, actor_id=048306667ae97d4c85a482aa01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f0f70152560>)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 32, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ray/train/huggingface/transformers/transformers_trainer.py\", line 482, in _huggingface_train_loop_per_worker\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1543, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1612, in _inner_training_loop\n",
      "    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/deepspeed.py\", line 344, in deepspeed_init\n",
      "    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/__init__.py\", line 165, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 308, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1162, in _configure_optimizer\n",
      "    basic_optimizer = self._configure_basic_optimizer(model_parameters)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1218, in _configure_basic_optimizer\n",
      "    optimizer = DeepSpeedCPUAdam(model_parameters,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/ops/adam/cpu_adam.py\", line 94, in __init__\n",
      "    self.ds_opt_adam = CPUAdamBuilder().load()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 445, in load\n",
      "    return self.jit_load(verbose)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 480, in jit_load\n",
      "    op_module = load(name=self.name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1284, in load\n",
      "    return _jit_compile(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1508, in _jit_compile\n",
      "    _write_ninja_file_and_build_library(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1623, in _write_ninja_file_and_build_library\n",
      "    _run_ninja_build(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1916, in _run_ninja_build\n",
      "    raise RuntimeError(message) from e\n",
      "RuntimeError: Error building extension 'cpu_adam'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m [2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=29899)\u001b[0m ninja: build stopped: subcommand failed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>date               </th><th>hostname    </th><th>node_ip   </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TransformersTrainer_59b78_00000</td><td>2023-09-21_19-37-56</td><td>56a026cbdb4c</td><td>172.17.0.4</td><td style=\"text-align: right;\">29764</td><td style=\"text-align: right;\"> 1695325076</td><td>59b78_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 19:39:47,705\tERROR tune.py:1107 -- Trials did not complete: [TransformersTrainer_59b78_00000]\n",
      "2023-09-21 19:39:47,706\tINFO tune.py:1111 -- Total run time: 116.95 seconds (116.94 seconds for the tuning loop).\n",
      "2023-09-21 19:39:47,708\tWARNING experiment_analysis.py:910 -- Failed to read the results for 1 trials:\n",
      "- /root/ray_results/TransformersTrainer_2023-09-21_19-37-50/TransformersTrainer_59b78_00000_0_2023-09-21_19-37-50\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TransformersTrainer.restore(\"/root/ray_results/TransformersTrainer_2023-09-21_19-37-50\")`.\nTo start a new run that will retry on training failures, set `air.RunConfig(failure_config=air.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=29764, ip=172.17.0.4, actor_id=998d5bc38c16d678c5d43ca701000000, repr=TransformersTrainer)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n    ray.get(object_ref)\nray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=29899, ip=172.17.0.4, actor_id=048306667ae97d4c85a482aa01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f0f70152560>)\n  File \"/opt/conda/lib/python3.10/subprocess.py\", line 526, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=29899, ip=172.17.0.4, actor_id=048306667ae97d4c85a482aa01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f0f70152560>)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 32, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n    train_func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/ray/train/huggingface/transformers/transformers_trainer.py\", line 482, in _huggingface_train_loop_per_worker\n    trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1543, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1612, in _inner_training_loop\n    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/deepspeed.py\", line 344, in deepspeed_init\n    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/__init__.py\", line 165, in initialize\n    engine = DeepSpeedEngine(args=args,\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 308, in __init__\n    self._configure_optimizer(optimizer, model_parameters)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1162, in _configure_optimizer\n    basic_optimizer = self._configure_basic_optimizer(model_parameters)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1218, in _configure_basic_optimizer\n    optimizer = DeepSpeedCPUAdam(model_parameters,\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/ops/adam/cpu_adam.py\", line 94, in __init__\n    self.ds_opt_adam = CPUAdamBuilder().load()\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 445, in load\n    return self.jit_load(verbose)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/ops/op_builder/builder.py\", line 480, in jit_load\n    op_module = load(name=self.name,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1284, in load\n    return _jit_compile(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1508, in _jit_compile\n    _write_ninja_file_and_build_library(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1623, in _write_ninja_file_and_build_library\n    _run_ninja_build(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1916, in _run_ninja_build\n    raise RuntimeError(message) from e\nRuntimeError: Error building extension 'cpu_adam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ray/train/base_trainer.py:616\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m result \u001b[38;5;241m=\u001b[39m result_grid[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([restore_msg, TrainingFailedError\u001b[38;5;241m.\u001b[39m_FAILURE_CONFIG_MSG])\n\u001b[1;32m    618\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresult\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTrainingFailedError\u001b[0m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TransformersTrainer.restore(\"/root/ray_results/TransformersTrainer_2023-09-21_19-37-50\")`.\nTo start a new run that will retry on training failures, set `air.RunConfig(failure_config=air.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
