{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042fef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - mpi4py\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         123 KB\n",
      "    certifi-2023.7.22          |  py310h06a4308_0         153 KB\n",
      "    conda-23.7.4               |  py310h06a4308_0         1.0 MB\n",
      "    libgfortran-ng-7.5.0       |      ha8ba4b0_17          22 KB\n",
      "    libgfortran4-7.5.0         |      ha8ba4b0_17         995 KB\n",
      "    mpi-1.0                    |            mpich          13 KB\n",
      "    mpi4py-3.1.4               |  py310hfc96bbd_0         573 KB\n",
      "    mpich-3.3.2                |       hc856adb_0         3.8 MB\n",
      "    openssl-1.1.1w             |       h7f8727e_0         3.7 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.5.0-ha8ba4b0_17 \n",
      "  libgfortran4       pkgs/main/linux-64::libgfortran4-7.5.0-ha8ba4b0_17 \n",
      "  mpi                pkgs/main/linux-64::mpi-1.0-mpich \n",
      "  mpi4py             pkgs/main/linux-64::mpi4py-3.1.4-py310hfc96bbd_0 \n",
      "  mpich              pkgs/main/linux-64::mpich-3.3.2-hc856adb_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                     2023.01.10-h06a4308_0 --> 2023.08.22-h06a4308_0 \n",
      "  certifi                          2023.5.7-py310h06a4308_0 --> 2023.7.22-py310h06a4308_0 \n",
      "  conda                              23.3.1-py310h06a4308_0 --> 23.7.4-py310h06a4308_0 \n",
      "  openssl                                 1.1.1t-h7f8727e_0 --> 1.1.1w-h7f8727e_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1w       | 3.7 MB    |                                       |   0% \n",
      "conda-23.7.4         | 1.0 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "mpich-3.3.2          | 3.8 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 123 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-ng-7.5.0 | 22 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 995 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi-1.0              | 13 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | #2                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 123 KB    | ############################9         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    | #######################2              |  63% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2023 | 123 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "conda-23.7.4         | 1.0 MB    | 5                                     |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-ng-7.5.0 | 22 KB     | ##########################9           |  73% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 995 KB    | 5                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi-1.0              | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | ######################9               |  62% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-ng-7.5.0 | 22 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi-1.0              | 13 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 573 KB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "mpich-3.3.2          | 3.8 MB    | #########6                            |  26% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 995 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran4-7.5.0   | 995 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 573 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mpi4py-3.1.4         | 573 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "conda-23.7.4         | 1.0 MB    | ##################################### | 100% \u001b[A\n",
      "conda-23.7.4         | 1.0 MB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "mpich-3.3.2          | 3.8 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    " #!pip install mpi4py\n",
    "!conda install mpi4py -y # if pip install mpi4py doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c348ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[torch]\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.0)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.29.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.65.0)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: tokenizers, safetensors, regex, fsspec, huggingface-hub, transformers, accelerate\n",
      "Successfully installed accelerate-0.23.0 fsspec-2023.9.2 huggingface-hub-0.17.2 regex-2023.8.8 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U torch accelerate transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6da1263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeed==0.9.5\n",
      "  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting xformers==0.0.21\n",
      "  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers==0.13.3 in /opt/conda/lib/python3.10/site-packages (0.13.3)\n",
      "Collecting datasets==2.8.0\n",
      "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (5.9.0)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic<2.0.0\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.8.0) (0.17.2)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.8.0) (2023.9.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.8.0) (2.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.8.0) (6.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.1.2)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.8.0) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.8.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.8.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.8.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.8.0) (1.26.15)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.8.0) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.8.0) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed==0.9.5) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.9.5) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844538 sha256=a817c2d16fd7597a99bf742fc9610a955d8c739d7863fa475487378e006a20f1\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, xxhash, tzdata, pydantic, pyarrow, protobuf, multidict, frozenlist, dill, async-timeout, yarl, tensorboardX, responses, pandas, multiprocess, aiosignal, xformers, deepspeed, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.8.0 deepspeed-0.9.5 dill-0.3.6 frozenlist-1.4.0 hjson-3.1.0 multidict-6.0.4 multiprocess-0.70.14 ninja-1.11.1 pandas-2.1.1 protobuf-4.24.3 py-cpuinfo-9.0.0 pyarrow-13.0.0 pydantic-1.10.12 responses-0.18.0 tensorboardX-2.6.2.2 tzdata-2023.3 xformers-0.0.21 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install deepspeed==0.9.5 xformers==0.0.21 tokenizers==0.13.3  datasets==2.8.0 tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cc9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "ROOT_PATH = \"./llama-2-fine-tune\"\n",
    "CONFIG_FILES_PATH = \"./llama-2-fine-tune/config/\"\n",
    "LOCAL_OUTPUT_DIR = \"./llama-2-fine-tune/output\"\n",
    "FINAL_MODEL_PATH = \"./final_model\"\n",
    "os.makedirs(ROOT_PATH,exist_ok=True)\n",
    "os.makedirs(CONFIG_FILES_PATH,exist_ok=True)\n",
    "os.makedirs(LOCAL_OUTPUT_DIR,exist_ok=True)\n",
    "os.makedirs(FINAL_MODEL_PATH,exist_ok=True)\n",
    "os.makedirs(\"./hf\",exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = \"./hf\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./hf\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./hf\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12612b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'daryl149/llama-2-7b-hf'\n",
    "TOKENIZER_PATH = 'daryl149/llama-2-7b-hf'\n",
    "DEFAULT_TRAINING_DATASET = \"mosaicml/dolly_hhrlhf\"\n",
    "CONFIG_PATH = \"./llama-2-fine-tune/config/a10_config.json\"\n",
    "DEFAULT_SEED = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42736310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": False\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"sub_group_size\": 5e7,\n",
    "        \"reduce_bucket_size\": \"auto\",\n",
    "        \"reduce_scatter\": True,\n",
    "        \"stage3_max_live_parameters\": 1e9,\n",
    "        \"stage3_max_reuse_distance\": 1e9,\n",
    "        \"stage3_prefetch_bucket_size\": 5e8,\n",
    "        \"stage3_param_persistence_threshold\": 1e6,\n",
    "        \"stage3_gather_16bit_weights_on_model_save\": True,\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        }\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 50,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": False\n",
    "}\n",
    "\n",
    "# data[\"zero_optimization\"][\"offload_param\"][\"device\"] = \"none\"\n",
    "# data[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n",
    "\n",
    "\n",
    "with open(CONFIG_PATH, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b517eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 07:34:20 INFO [torch.distributed.nn.jit.instantiator] Created a temporary directory at /tmp/tmpg27ihsh4\n",
      "2023-09-24 07:34:20 INFO [torch.distributed.nn.jit.instantiator] Writing /tmp/tmpg27ihsh4/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import field, dataclass\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "import transformers\n",
    "from accelerate.utils import DistributedType\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    IntervalStrategy,\n",
    "    PreTrainedTokenizer,\n",
    "    SchedulerType,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ecb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HFTrainingArguments:\n",
    "    local_rank: Optional[str] = field(default=\"-1\")\n",
    "    dataset: Optional[str] = field(default=DEFAULT_TRAINING_DATASET)\n",
    "    model: Optional[str] = field(default=MODEL_PATH)\n",
    "    tokenizer: Optional[str] = field(default=TOKENIZER_PATH)\n",
    "    max_seq_len: Optional[int] = field(default=256)\n",
    "\n",
    "    final_model_output_path: Optional[str] = field(default = FINAL_MODEL_PATH)\n",
    "\n",
    "    deepspeed_config: Optional[str] = field(default=CONFIG_PATH)\n",
    "\n",
    "    output_dir: Optional[str] = field(default=LOCAL_OUTPUT_DIR)\n",
    "    per_device_train_batch_size: Optional[int] = field(default=1)\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1)\n",
    "    gradient_checkpointing: Optional[bool] = field(default=True)\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=1)\n",
    "    learning_rate: Optional[float] = field(default=1e-6)\n",
    "    optim: Optional[str] = field(default=\"adamw_hf\")\n",
    "    num_train_epochs: Optional[int] = field(default=None)\n",
    "    max_steps: Optional[int] = field(default=-1)\n",
    "    adam_beta1: float = field(default=0.9)\n",
    "    adam_beta2: float = field(default=0.999)\n",
    "    adam_epsilon: float = field(default=1e-8)\n",
    "    lr_scheduler_type: Union[SchedulerType, str] = field(\n",
    "        default=\"cosine\",\n",
    "    )\n",
    "    warmup_steps: int = field(default=0)\n",
    "    weight_decay: Optional[float] = field(default=1)\n",
    "    logging_strategy: Optional[Union[str, IntervalStrategy]] = field(\n",
    "        default=IntervalStrategy.STEPS\n",
    "    )\n",
    "    evaluation_strategy: Optional[Union[str, IntervalStrategy]] = field(\n",
    "        default=IntervalStrategy.STEPS\n",
    "    )\n",
    "    save_strategy: Optional[Union[str, IntervalStrategy]] = field(\n",
    "        default=IntervalStrategy.STEPS\n",
    "    )\n",
    "    fp16: Optional[bool] = field(default=False)\n",
    "    bf16: Optional[bool] = field(default=True)\n",
    "    save_steps: Optional[int] = field(default=100)\n",
    "    logging_steps: Optional[int] = field(default=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14250c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_dataset(\n",
    "    tokenizer,\n",
    "    path_or_dataset: str = DEFAULT_TRAINING_DATASET,\n",
    "    max_seq_len: int = 256,\n",
    "    seed: int = DEFAULT_SEED,\n",
    ") -> Dataset:\n",
    "    logger.info(f\"Loading dataset from {path_or_dataset}\")\n",
    "    dataset = load_dataset(path_or_dataset)\n",
    "    logger.info(f\"Training: found {dataset['train'].num_rows} rows\")\n",
    "    logger.info(f\"Eval: found {dataset['test'].num_rows} rows\")\n",
    "\n",
    "    # Reformat input data, add prompt template if needed\n",
    "    def _reformat_data(row):\n",
    "        return row[\"prompt\"] + row[\"response\"]\n",
    "\n",
    "    # Inspired from: https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt\n",
    "    def tokenize(element):\n",
    "        input_batch = []\n",
    "        attention_masks = []\n",
    "\n",
    "        outputs = tokenizer(\n",
    "            _reformat_data(element),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_seq_len,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=True,\n",
    "        )\n",
    "\n",
    "        for length, input_ids, attention_mask in zip(\n",
    "            outputs[\"length\"], outputs[\"input_ids\"], outputs[\"attention_mask\"]\n",
    "        ):\n",
    "            if length == max_seq_len:\n",
    "                input_batch.append(input_ids)\n",
    "                attention_masks.append(attention_mask)\n",
    "\n",
    "        return {\"input_ids\": input_batch, \"attention_mask\": attention_masks}\n",
    "\n",
    "    train_tokenized_dataset = dataset[\"train\"].map(\n",
    "        tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    eval_tokenized_dataset = dataset[\"test\"].map(\n",
    "        tokenize, batched=True, remove_columns=dataset[\"test\"].column_names\n",
    "    )\n",
    "\n",
    "    return train_tokenized_dataset, eval_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc27633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    pretrained_name_or_path: str = MODEL_PATH\n",
    ") -> AutoModelForCausalLM:\n",
    "    logger.info(f\"Loading model: {pretrained_name_or_path}\")\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_name_or_path,\n",
    "        trust_remote_code=\"true\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map= None,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e27cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(\n",
    "    pretrained_name_or_path: str,\n",
    ") -> PreTrainedTokenizer:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_name_or_path, trust_remote_code=\"true\", padding_side=\"left\"\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e98b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args: HFTrainingArguments):\n",
    "    set_seed(DEFAULT_SEED)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    tokenizer = get_tokenizer(args.tokenizer)\n",
    "    train_dataset, eval_dataset = load_training_dataset(\n",
    "    tokenizer, path_or_dataset=args.dataset, max_seq_len=args.max_seq_len\n",
    "    )\n",
    "    model = get_model(pretrained_name_or_path=args.model)\n",
    "\n",
    "    if args.deepspeed_config:\n",
    "        with open(args.deepspeed_config) as json_data:\n",
    "            deepspeed_config_dict = json.load(json_data)\n",
    "    else:\n",
    "        deepspeed_config_dict = None\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    local_rank=args.local_rank,\n",
    "    output_dir=args.output_dir,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "    gradient_checkpointing=args.gradient_checkpointing,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    learning_rate=args.learning_rate,\n",
    "    optim=args.optim,\n",
    "    num_train_epochs=args.num_train_epochs,\n",
    "    max_steps=args.max_steps,\n",
    "    adam_beta1=args.adam_beta1,\n",
    "    adam_beta2=args.adam_beta2,\n",
    "    adam_epsilon=args.adam_epsilon,\n",
    "    lr_scheduler_type=args.lr_scheduler_type,\n",
    "    warmup_steps=args.warmup_steps,\n",
    "    weight_decay=args.weight_decay,\n",
    "    logging_strategy=args.logging_strategy,\n",
    "    evaluation_strategy=args.evaluation_strategy,\n",
    "    save_strategy=args.save_strategy,\n",
    "    fp16=args.fp16,\n",
    "    bf16=args.bf16,\n",
    "    deepspeed=deepspeed_config_dict,\n",
    "    save_steps=args.save_steps,\n",
    "    logging_steps=args.logging_steps,\n",
    "    push_to_hub=False,\n",
    "    disable_tqdm=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    # group_by_length=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    # fsdp=[\"full_shard\", \"offload\"],\n",
    "    )\n",
    "\n",
    "    # set distributed_state manually\n",
    "    training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Training the model\")\n",
    "    trainer.train()\n",
    "\n",
    "    logger.info(f\"Saving Model to {args.final_model_output_path}\")\n",
    "    trainer.save_model(output_dir=args.final_model_output_path)\n",
    "    tokenizer.save_pretrained(args.final_model_output_path)\n",
    "\n",
    "    logger.info(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e417df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9906515170d41f5acb1da5aee535a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb36cb5dc8ec47baa46c4dcdea66358f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5094cc25017b4792ac2892cb44862610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0b10d9ea374cb082fa22e0354b10d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 07:34:23 INFO [__main__] Loading dataset from mosaicml/dolly_hhrlhf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b27b39f1584a098e04432b3bd3415a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 07:34:24 WARNING [datasets.builder] Using custom data configuration mosaicml--dolly_hhrlhf-9d0c74ee24e2b1d0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/hf/mosaicml___parquet/mosaicml--dolly_hhrlhf-9d0c74ee24e2b1d0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e5f960e40a4e75a163875e09c1dd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b9a6c80bcf451d8edc4bc3c3c6714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.77M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed9f990959a41b1aa0b0c57f2420ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2d1b83b3a1457ca08bceb224d84c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/59310 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/hf/mosaicml___parquet/mosaicml--dolly_hhrlhf-9d0c74ee24e2b1d0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef478a122da4bcfabbd624ddcaee13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 07:34:29 INFO [__main__] Training: found 59310 rows\n",
      "2023-09-24 07:34:29 INFO [__main__] Eval: found 5129 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9042b0b084c84f41adeb779edf1e74e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3996c8d85a49e2ace3b137a426f02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 07:34:44 INFO [__main__] Loading model: daryl149/llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bd3bfbd6f14959a4c1eb34e5c25cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7681adeb254e5ab77404fdce86b20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4e74f86ca1473aab70dea39c2567c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a511b6ea8d76400cb24ee07093bc2703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850ee085ab34414eb52cb3d190585c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df924ef36d44fcd94775879bc87b51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcabac0bea74de39f781e26a9bbcea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 07:41:30 INFO [__main__] Training the model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-24 07:41:30,291] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-09-24 07:41:30,457] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 07:41:31 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-09-24 07:41:31 INFO [torch.distributed.distributed_c10d] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "2023-09-24 07:41:39 INFO [torch.distributed.distributed_c10d] Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-09-24 07:41:39 INFO [torch.distributed.distributed_c10d] Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "Using /root/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py310_cu117/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Time to load cpu_adam op: 30.663453817367554 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.072, 'learning_rate': 1e-06, 'epoch': 0.0}\n",
      "{'eval_loss': 2.127370595932007, 'eval_runtime': 381.1694, 'eval_samples_per_second': 26.912, 'eval_steps_per_second': 1.123, 'epoch': 0.0}\n",
      "{'loss': 1.9329, 'learning_rate': 1e-06, 'epoch': 0.0}\n",
      "{'eval_loss': 1.63425612449646, 'eval_runtime': 389.8742, 'eval_samples_per_second': 26.311, 'eval_steps_per_second': 1.098, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def main(args_list):\n",
    "    parser = HfArgumentParser(HFTrainingArguments)\n",
    "\n",
    "    parsed = parser.parse_args_into_dataclasses(args_list)\n",
    "    args: HFTrainingArguments = parsed[0]\n",
    "\n",
    "    train(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"HF_HOME\"] = \"./hf\"\n",
    "    os.environ[\"HF_DATASETS_CACHE\"] = \"./hf\"\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = \"./hf\"\n",
    "\n",
    "    # List of arguments similar to the command-line arguments\n",
    "    cmd_args = [\n",
    "        \"--final_model_output_path\", \"./final_model\",\n",
    "        \"--output_dir\", \"./llama-2-fine-tune/output\",\n",
    "        \"--dataset\", \"mosaicml/dolly_hhrlhf\",\n",
    "        \"--model\", \"daryl149/llama-2-7b-hf\",\n",
    "        \"--tokenizer\", \"daryl149/llama-2-7b-hf\",\n",
    "        \"--deepspeed_config\", \"./llama-2-fine-tune/config/a10_config.json\",\n",
    "        \"--fp16\", \"false\",\n",
    "        \"--bf16\", \"true\",\n",
    "        \"--per_device_train_batch_size\", \"24\",\n",
    "        \"--per_device_eval_batch_size\", \"24\",\n",
    "        \"--gradient_checkpointing\", \"true\",\n",
    "        \"--gradient_accumulation_steps\", \"1\",\n",
    "        \"--save_steps\", \"500\",\n",
    "        \"--num_train_epochs\", \"1\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        main(cmd_args)\n",
    "    except Exception:\n",
    "        logger.exception(\"main failed\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f04673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
